import torch


def model_loss_mse(model, X, Y):
    """
    Implement the MSE part of the loss
    The model prediction is compared to values generated by a finit volume fluid simulation

    Arguments :
        X = coordinates from the interior of the domain
        Y = values on the interior of the domain
        model = the model that we train
    Return :
        MSE
    """
    
    predictions = model(X)
    
    return torch.mean((Y - predictions)**2)




def model_loss_equation(model, X, rho, nu):
    """
    Implement the part of the loss that makes sure the NN satisfies the time dependent equation on the domain

    Arguments :
        X = inputs form the dataset
        model = the model that we train
        rho = density (float)
        nu = kinematic viscosity (float)
    Return :
        MSE on the pde
    """
    
    Y_pred = model(X)

    u = Y_pred[:, 0]
    v = Y_pred[:, 1]
    p = Y_pred[:, 2]

    u_x = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 0]
    u_y = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 1]
    v_x = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 0]
    v_y = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 1]
    p_x = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 0]
    p_y = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 1]

    u_t = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 2]
    v_t = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 2]

    u_xx = torch.autograd.grad(u_x, X, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0][:, 0]
    u_yy = torch.autograd.grad(u_y, X, grad_outputs=torch.ones_like(u_y), retain_graph=True, create_graph=True)[0][:, 1]
    v_xx = torch.autograd.grad(v_x, X, grad_outputs=torch.ones_like(v_x), retain_graph=True, create_graph=True)[0][:, 0]
    v_yy = torch.autograd.grad(v_y, X, grad_outputs=torch.ones_like(v_y), retain_graph=True, create_graph=True)[0][:, 1]

    #equations
    e1 = u_t +  u*u_x + v*u_y + p_x/rho - nu*(u_xx + u_yy)
    e2 = v_t + u*v_x + v*v_y + p_y/rho - nu*(v_xx + v_yy)
    e3 = u_x + v_y  # null divergence

    #compute the loss as the mean squared error of the equations
    loss = torch.mean(e1**2 + e2**2 + e3**2)
    return loss



def model_loss_equation_kovasznay(model, X, nu):
    """
    Implement the part of the loss that makes sure the NN satisfies the  time independent equation on the domain

    Arguments :
        X = inputs form the dataset
        model = the model that we train
        nu = viscosity (float)
    Return :
        MSE on the pde
    """
    
    Y_pred = model(X)

    u = Y_pred[:, 0]
    v = Y_pred[:, 1]
    p = Y_pred[:, 2]

    u_x = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 0]
    u_y = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 1]
    v_x = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 0]
    v_y = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 1]
    p_x = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 0]
    p_y = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 1]

    u_xx = torch.autograd.grad(u_x, X, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0][:, 0]
    u_yy = torch.autograd.grad(u_y, X, grad_outputs=torch.ones_like(u_y), retain_graph=True, create_graph=True)[0][:, 1]
    v_xx = torch.autograd.grad(v_x, X, grad_outputs=torch.ones_like(v_x), retain_graph=True, create_graph=True)[0][:, 0]
    v_yy = torch.autograd.grad(v_y, X, grad_outputs=torch.ones_like(v_y), retain_graph=True, create_graph=True)[0][:, 1]

    #equations
    e1 = u*u_x + v*u_y + p_x - nu*(u_xx + u_yy)
    e2 = u*v_x + v*v_y + p_y - nu*(v_xx + v_yy)
    e3 = u_x + v_y  #zero divergence

    #compute the loss as the mean squared error of the equations
    loss = torch.mean(e1**2 + e2**2 + e3**2)
    return loss



def model_loss_boundary_kovasznay(model, X_left, X_right, X_bot, X_top, Y_left, Y_right, Y_bot, Y_top):
    """
    Implement the part of the loss that makes sure the NN satifies the boundary conditions

    Arguments :
        X_left = left side of the domain
        X_right = right side of the domain
        X_top = top side of the domain
        X_bot = bottom side of the domain
        Y_left = boundary condition on the left side of the domain
        Y_right = boundary condition on the right side of the domain
        Y_top = boundary condition on the top side of the domain
        Y_bot = boundary condition on the bottom side of the domain
        model = the model that we train
    Return :
        MSE on each side of the domain
    """

    Y_pred_bot = model(X_bot)
    Y_pred_top = model(X_top)
    Y_pred_left = model(X_left)
    Y_pred_right = model(X_right)

    e_left = Y_pred_left - Y_left
    e_right = Y_pred_right - Y_right 
    e_top = Y_pred_top - Y_top
    e_bot = Y_pred_bot - Y_bot

    return torch.mean(e_left**2 + e_right**2 + e_bot**2 + e_top**2)


